{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CycleGAN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/bkkaggle/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"5VIGyIus8Vr7"},"source":["Take a look at the [repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) for more information"]},{"cell_type":"markdown","metadata":{"id":"7wNjDKdQy35h"},"source":["# Install"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtV9HyDxlvpJ","executionInfo":{"status":"ok","timestamp":1618713361339,"user_tz":-480,"elapsed":61120,"user":{"displayName":"Kevyn ZHANG","photoUrl":"","userId":"08822570782718335276"}},"outputId":"b29336f6-9919-4312-99cf-97e902c48e27"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Colab Notebooks"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TRm-USlsHgEV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618678581577,"user_tz":-480,"elapsed":1177,"user":{"displayName":"Kevyn ZHANG","photoUrl":"","userId":"08822570782718335276"}},"outputId":"7f9ef4ef-7cb5-42eb-d0dd-a41df497bbec"},"source":["!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"],"execution_count":2,"outputs":[{"output_type":"stream","text":["fatal: destination path 'pytorch-CycleGAN-and-pix2pix' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pt3igws3eiVp","executionInfo":{"status":"ok","timestamp":1618713364680,"user_tz":-480,"elapsed":2167,"user":{"displayName":"Kevyn ZHANG","photoUrl":"","userId":"08822570782718335276"}}},"source":["import os\n","os.chdir('pytorch-CycleGAN-and-pix2pix/')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1EySlOXwwoa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618713378001,"user_tz":-480,"elapsed":12410,"user":{"displayName":"Kevyn ZHANG","photoUrl":"","userId":"08822570782718335276"}},"outputId":"eb27f004-9448-4cd5-adfa-f4d14a972702"},"source":["!pip install -r requirements.txt"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.8.1+cu101)\n","Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.9.1+cu101)\n","Collecting dominate>=2.4.0\n","  Downloading https://files.pythonhosted.org/packages/ef/a8/4354f8122c39e35516a2708746d89db5e339c867abbd8e0179bccee4b7f9/dominate-2.6.0-py2.py3-none-any.whl\n","Collecting visdom>=0.1.8.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/75/e078f5a2e1df7e0d3044749089fc2823e62d029cc027ed8ae5d71fafcbdc/visdom-0.1.8.9.tar.gz (676kB)\n","\u001b[K     |████████████████████████████████| 686kB 13.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.7.4.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (7.1.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.23.0)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (5.1.1)\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (22.0.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.15.0)\n","Collecting jsonpatch\n","  Downloading https://files.pythonhosted.org/packages/a3/55/f7c93bae36d869292aedfbcbae8b091386194874f16390d680136edd2b28/jsonpatch-1.32-py2.py3-none-any.whl\n","Collecting torchfile\n","  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n","Collecting websocket-client\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/33/80e0d4f60e84a1ddd9a03f340be1065a2a363c47ce65c4bd3bae65ce9631/websocket_client-0.58.0-py2.py3-none-any.whl (61kB)\n","\u001b[K     |████████████████████████████████| 61kB 9.5MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.24.3)\n","Collecting jsonpointer>=1.9\n","  Downloading https://files.pythonhosted.org/packages/23/52/05f67532aa922e494c351344e0d9624a01f74f5dd8402fe0d1b563a6e6fc/jsonpointer-2.1-py2.py3-none-any.whl\n","Building wheels for collected packages: visdom, torchfile\n","  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for visdom: filename=visdom-0.1.8.9-cp37-none-any.whl size=655251 sha256=01b96261c334fa2e986d25e5c424d12471d9326ce98becba2d308c1af4539ce3\n","  Stored in directory: /root/.cache/pip/wheels/70/19/a7/6d589ed967f4dfefd33bc166d081257bd4ed0cb618dccfd62a\n","  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchfile: filename=torchfile-0.1.0-cp37-none-any.whl size=5713 sha256=ce863e9ae481336564da7e48fc83a61875234fc447a036ee14feee255ef25117\n","  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n","Successfully built visdom torchfile\n","Installing collected packages: dominate, jsonpointer, jsonpatch, torchfile, websocket-client, visdom\n","Successfully installed dominate-2.6.0 jsonpatch-1.32 jsonpointer-2.1 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-0.58.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yFw1kDQBx3LN"},"source":["# Training\n","\n","-   `python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model cycle_gan`\n","\n","Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. I've found that a batch size of 16 fits onto 4 V100s and can finish training an epoch in ~90s.\n","\n","Once your model has trained, copy over the last checkpoint to a format that the testing model can automatically detect:\n","\n","Use `cp ./checkpoints/horse2zebra/latest_net_G_A.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class A to class B and `cp ./checkpoints/horse2zebra/latest_net_G_B.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class B to class A.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jLQWF9E3nOuR","executionInfo":{"status":"ok","timestamp":1618713382122,"user_tz":-480,"elapsed":1388,"user":{"displayName":"Kevyn ZHANG","photoUrl":"","userId":"08822570782718335276"}},"outputId":"29f8d53f-2311-433e-8df1-dd274edab289"},"source":["%ls -al"],"execution_count":4,"outputs":[{"output_type":"stream","text":["total 161\n","drwx------ 2 root root  4096 Apr 17 11:35 \u001b[0m\u001b[01;34mcheckpoints\u001b[0m/\n","-rw------- 1 root root 77365 Apr 18 02:36 CycleGAN.ipynb\n","drwx------ 2 root root  4096 Apr 17 10:59 \u001b[01;34mdata\u001b[0m/\n","drwx------ 2 root root  4096 Apr 17 10:59 \u001b[01;34mdatasets\u001b[0m/\n","drwx------ 2 root root  4096 Apr 17 10:59 \u001b[01;34mdocs\u001b[0m/\n","-rw------- 1 root root   235 Apr 17 10:59 environment.yml\n","drwx------ 2 root root  4096 Apr 17 10:59 \u001b[01;34m.git\u001b[0m/\n","-rw------- 1 root root   789 Apr 17 10:59 .gitignore\n","drwx------ 2 root root  4096 Apr 17 10:59 \u001b[01;34mimgs\u001b[0m/\n","-rw------- 1 root root  3623 Apr 17 10:59 LICENSE\n","drwx------ 2 root root  4096 Apr 17 10:59 \u001b[01;34mmodels\u001b[0m/\n","drwx------ 2 root root  4096 Apr 17 10:59 \u001b[01;34moptions\u001b[0m/\n","-rw------- 1 root root  7727 Apr 17 10:59 pix2pix.ipynb\n","-rw------- 1 root root 16087 Apr 17 10:59 README.md\n","-rw------- 1 root root   796 Apr 17 10:59 .replit\n","-rw------- 1 root root    68 Apr 17 10:59 requirements.txt\n","drwx------ 2 root root  4096 Apr 17 15:01 \u001b[01;34mresults\u001b[0m/\n","drwx------ 2 root root  4096 Apr 17 10:59 \u001b[01;34mscripts\u001b[0m/\n","-rw------- 1 root root  4222 Apr 17 10:59 test.py\n","-rw------- 1 root root  5010 Apr 17 10:59 train.py\n","drwx------ 2 root root  4096 Apr 17 10:59 \u001b[01;34mutil\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0sp7TCT2x9dB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618713392778,"user_tz":-480,"elapsed":4045,"user":{"displayName":"Kevyn ZHANG","photoUrl":"","userId":"08822570782718335276"}},"outputId":"4f75431c-03da-4b97-f6e4-3450ffddfc9d"},"source":["!python train.py --dataroot ./datasets/GTAV_2_Real --name GTAV2Real --model cycle_gan"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"train.py\", line 22, in <module>\n","    from options.train_options import TrainOptions\n","  File \"/content/drive/My Drive/Colab Notebooks/pytorch-CycleGAN-and-pix2pix/options/train_options.py\", line 1, in <module>\n","    from .base_options import BaseOptions\n","  File \"/content/drive/My Drive/Colab Notebooks/pytorch-CycleGAN-and-pix2pix/options/base_options.py\", line 5, in <module>\n","    import models\n","  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n","  File \"<frozen importlib._bootstrap_external>\", line 818, in get_code\n","  File \"<frozen importlib._bootstrap_external>\", line 916, in get_data\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9UkcaFZiyASl"},"source":["# Testing\n","\n","-   `python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout`\n","\n","Change the `--dataroot` and `--name` to be consistent with your trained model's configuration.\n","\n","> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n","> The option --model test is used for generating results of CycleGAN only for one side. This option will automatically set --dataset_mode single, which only loads the images from one set. On the contrary, using --model cycle_gan requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at ./results/. Use --results_dir {directory_path_to_save_result} to specify the results directory.\n","\n","> For your own experiments, you might want to specify --netG, --norm, --no_dropout to match the generator architecture of the trained model."]},{"cell_type":"code","metadata":{"id":"uCsKkEq0yGh0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618714289711,"user_tz":-480,"elapsed":8891,"user":{"displayName":"Kevyn ZHANG","photoUrl":"","userId":"08822570782718335276"}},"outputId":"691fad83-1ef5-48b7-8e00-cf5b987268d9"},"source":["!python test.py --dataroot datasets/GTAV_2_Real/testA --name GTAV2Real --model test --no_dropout"],"execution_count":21,"outputs":[{"output_type":"stream","text":["----------------- Options ---------------\n","             aspect_ratio: 1.0                           \n","               batch_size: 1                             \n","          checkpoints_dir: ./checkpoints                 \n","                crop_size: 256                           \n","                 dataroot: datasets/GTAV_2_Real/testA    \t[default: None]\n","             dataset_mode: single                        \n","                direction: AtoB                          \n","          display_winsize: 256                           \n","                    epoch: latest                        \n","                     eval: False                         \n","                  gpu_ids: 0                             \n","                init_gain: 0.02                          \n","                init_type: normal                        \n","                 input_nc: 3                             \n","                  isTrain: False                         \t[default: None]\n","                load_iter: 0                             \t[default: 0]\n","                load_size: 256                           \n","         max_dataset_size: inf                           \n","                    model: test                          \n","             model_suffix:                               \n","               n_layers_D: 3                             \n","                     name: GTAV2Real                     \t[default: experiment_name]\n","                      ndf: 64                            \n","                     netD: basic                         \n","                     netG: resnet_9blocks                \n","                      ngf: 64                            \n","               no_dropout: True                          \t[default: False]\n","                  no_flip: False                         \n","                     norm: instance                      \n","                 num_test: 50                            \n","              num_threads: 4                             \n","                output_nc: 3                             \n","                    phase: test                          \n","               preprocess: resize_and_crop               \n","              results_dir: ./results/                    \n","           serial_batches: False                         \n","                   suffix:                               \n","                  verbose: False                         \n","----------------- End -------------------\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","dataset [SingleDataset] was created\n","initialize network with normal\n","model [TestModel] was created\n","loading the model from ./checkpoints/GTAV2Real/latest_net_G.pth\n","---------- Networks initialized -------------\n","[Network G] Total number of parameters : 11.378 M\n","-----------------------------------------------\n","creating web directory ./results/GTAV2Real/test_latest\n","processing (0000)-th image... ['datasets/GTAV_2_Real/testA/1618489974.jpg']\n","processing (0005)-th image... ['datasets/GTAV_2_Real/testA/1618491048.jpg']\n","processing (0010)-th image... ['datasets/GTAV_2_Real/testA/1618494634.jpg']\n","processing (0015)-th image... ['datasets/GTAV_2_Real/testA/1618495925.jpg']\n","processing (0020)-th image... ['datasets/GTAV_2_Real/testA/1618497294.jpg']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f4Unyyk-4vn5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OzSKIPUByfiN"},"source":["# Visualize"]},{"cell_type":"code","metadata":{"id":"9Mgg8raPyizq"},"source":["import matplotlib.pyplot as plt\n","\n","img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_fake.png')\n","plt.imshow(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0G3oVH9DyqLQ"},"source":["import matplotlib.pyplot as plt\n","\n","img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_real.png')\n","plt.imshow(img)"],"execution_count":null,"outputs":[]}]}